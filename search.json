[
  {
    "objectID": "tips_tricks.html",
    "href": "tips_tricks.html",
    "title": "Tips and Tricks",
    "section": "",
    "text": "Tips and Tricks\nA few miscelaneous thought and observations from several years of using LandTrendr\n\nKeeping careful track of what parameters you used to generate which outputs can save you lots of headaches down the line as there is no way to tell from an output what parameters were used to generate it once it’s been spun up. We recommended both a consistent naming convention and saving changes to your script between each output as GEE keeps a version history of your scripts.\nWe have found that flucuations of water levels in wetlands areas often shows up disturbance in LT outputs, which depending on your purposes may not be ideal. Unfortunaltey these fluctuations do result in very real changes in spectral values, and closely resemble other types of changes that may be of interest.\nOn a related note, we highly recomend applying a water mask to your LT outputs as predictions for bodies of water are typically meaningless. Depeneding on the area that you are interested in, masking out developed areas may also be useful.\nFor those who may be interested in exploring LandTrendr without wading into running the algorithm themselves, the eMapR group have also developed several UI applications of LandTrendr that offer several different ways to interact with LandTrendr timeseries data.",
    "crumbs": [
      "Tips and Tricks"
    ]
  },
  {
    "objectID": "getting_started.html",
    "href": "getting_started.html",
    "title": "Getting Started",
    "section": "",
    "text": "Both the original LandTrendr paper Kennedy et al. 2010 and the GEE engine implementation paper Kennedy et al. 2018 are excellent jumping off points for understanding how the algorithm works.\nFor more specific information on LandTrendr in GEE, the holy grail is the eMapR LT GEE Guide. This document walks you through running LandTrendr in GEE step by step, with example code. Speaking of code, most of us in the CAFRI lab use the eMapR API code (or derivatives of it) when running LandTrendr. These scripts are the easiest place to start if you would like to run LandTrendr on your own. For quick reference, the GEE developers guide has good information about algorithm parameter defaults.\n\n\n\nThe LandTrendr algorithm has several parameters that a user can adjust to control how the algorithm segments the time series of spectral data. While each of these parameters serves a specific purpose, there are some that we have found to be more useful than others. For more detailed discussion of each parameter, and their sensitivity, see Kennedy et al. 2010. For information on adjusting these parameters for using Landtrendr in the Northern Forest Region see Algorithm Tuning\n\n\nThe recovery threshold parameter sets the minimum allowable length of time after a disturbance event for spectral recovery. That is, this parameter control what length of time is ‘too fast’ to see a recovery of spectral values. By default this time period is 4 years (recovery period is the inverse of recovery threshold, 0.25 = 4 year recovery period) which in the Northeast is quite a long time, as our forest disturbance events tend to be low intensity and there is often advanced regeneration ready to fill any new canopy gaps. This parameter is the first parameter that we adjust in the CAFRI lab, often setting it to 0.75 (see Algorithm Tuning for more).\n\n\n\nThis parameter is the only algorithm parameter without a default value as the appropriate number of segments depends on how long of a time period you are studying. For example, if you have a 3 year time period you wouldn’t want to have the same maximum number of segments as a 30 year time period, otherwise you would have a segmentation that is much more complex than would be reasonable for a short amount of time. Therefore the general rule is, the longer the time period, the more segments that you need.\n\n\n\n\nLandTrendr Segmentation Parameters\n\n\nParameter\nDescription\nDefault\n\n\n\n\nspike threshold\nSets the size of abrupt deviation in spectral value that is considered a spike. Smaller values filter out spikes more aggressively\n0.9\n\n\np-value threshold\nThe p-value threshold for trajectory selection. Trajectories with p-values that exceed this threshold are considered no change.\n0.05\n\n\nmax segments\nThe maximum number of segments allowed in trajectory fitting\n\n\n\nvertex count overshoot\nInitial regression-based detection of potential vertices can overshoot the number of vertices allowed by max segments by this value; angle-based culling is used to return to the desired number of vertices if overshoot occurs. This allows for a mix of criteria for vertex identification.\n3\n\n\nrecovery threshold\nDisallows candidate segment that have a recovery rate faster than 1/recovery threshold (in years).\n0.25\n\n\nprevent one year recovery\nDisallows all segments with recovery rates equal to one year, either true or false.\nFALSE\n\n\nbest model proportion\nAllows models with more vertices than allowed by max segments to be chosen if their p-value is no more than (2 – best model proportion) times the p-value of the best model.\n0.75\n\n\nminimum observations needed\nMinimum number of cloud free images needed to perform fitting.\n6\n\n\n\n\n\n\n\n\n\n\n\nThere is also a second set of parameters that are easily adjusted by the user that control which parts of the segmented time series are exported to a change map output. These parameters can greatly affect how your change map appears, and as such are important to understand. For more details on these parameters see the eMapR API documentation.\n\n\n\n\nLandTrendr Change Map Parameters\n\n\nParameter\nDescription\n\n\n\n\ndelta\nChange type to map – either loss (disturbance) or gain (recovery/growth).\n\n\nsort\nThe type of change to identify if there is more than one change event in the pixel of interest – one of greatest, least, newest, oldest, fastest, slowest.\n\n\nmagnitude\nEither a minimum or maximum value for filtering change events by change in spectral reflectance value; filtering is not required.\n\n\nduration\nEither a minimum or maximum value for filtering change events by length of event. Filtering by disturbance duration is not required.\n\n\npreval\nEither a minimum or maximum value for filtering change events by the pre-change value of the spectral index. Filtering by preval is not required.\n\n\nminimum mapping unit (mmu)\nIndicates the smallest allowable disturbance patch. Pixels disturbed in the same year and adjacent following an 8-neightbor rule are considered to be of the same patch. Filtering by mmu is not required.\n\n\n\n\n\n\n\n\n\n\n\n\nOne thing to be aware of when you are running LandTrendr is what imagery you are feeding into the algorithm. If you are using one of the eMapR scripts, the selection of imagery happens under the hood in a separate piece of code that is called to actually run the algorithm. As of April 2024, all the eMapR scripts default to Collection 2 Landsat imagery. For most users, who want to run Landtrendr on Landsat imagery, this is not a problem. However, if you want to use other timeseries data or if you are generating outputs over long periods of time, you may need to adjust this.\nAdditionally, if you are using the eMapR scripts, their API is designed to automatically be compatible with many common spectral indcies which makes it easy to switch between indices. In the CAFRI lab we almost always run LandTrendr on NBR, as we have found it to have the best performance.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "getting_started.html#recommended-papers-and-resources",
    "href": "getting_started.html#recommended-papers-and-resources",
    "title": "Getting Started",
    "section": "",
    "text": "Both the original LandTrendr paper Kennedy et al. 2010 and the GEE engine implementation paper Kennedy et al. 2018 are excellent jumping off points for understanding how the algorithm works.\nFor more specific information on LandTrendr in GEE, the holy grail is the eMapR LT GEE Guide. This document walks you through running LandTrendr in GEE step by step, with example code. Speaking of code, most of us in the CAFRI lab use the eMapR API code (or derivatives of it) when running LandTrendr. These scripts are the easiest place to start if you would like to run LandTrendr on your own. For quick reference, the GEE developers guide has good information about algorithm parameter defaults.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "getting_started.html#understanding-algorithm-parameters",
    "href": "getting_started.html#understanding-algorithm-parameters",
    "title": "Getting Started",
    "section": "",
    "text": "The LandTrendr algorithm has several parameters that a user can adjust to control how the algorithm segments the time series of spectral data. While each of these parameters serves a specific purpose, there are some that we have found to be more useful than others. For more detailed discussion of each parameter, and their sensitivity, see Kennedy et al. 2010. For information on adjusting these parameters for using Landtrendr in the Northern Forest Region see Algorithm Tuning\n\n\nThe recovery threshold parameter sets the minimum allowable length of time after a disturbance event for spectral recovery. That is, this parameter control what length of time is ‘too fast’ to see a recovery of spectral values. By default this time period is 4 years (recovery period is the inverse of recovery threshold, 0.25 = 4 year recovery period) which in the Northeast is quite a long time, as our forest disturbance events tend to be low intensity and there is often advanced regeneration ready to fill any new canopy gaps. This parameter is the first parameter that we adjust in the CAFRI lab, often setting it to 0.75 (see Algorithm Tuning for more).\n\n\n\nThis parameter is the only algorithm parameter without a default value as the appropriate number of segments depends on how long of a time period you are studying. For example, if you have a 3 year time period you wouldn’t want to have the same maximum number of segments as a 30 year time period, otherwise you would have a segmentation that is much more complex than would be reasonable for a short amount of time. Therefore the general rule is, the longer the time period, the more segments that you need.\n\n\n\n\nLandTrendr Segmentation Parameters\n\n\nParameter\nDescription\nDefault\n\n\n\n\nspike threshold\nSets the size of abrupt deviation in spectral value that is considered a spike. Smaller values filter out spikes more aggressively\n0.9\n\n\np-value threshold\nThe p-value threshold for trajectory selection. Trajectories with p-values that exceed this threshold are considered no change.\n0.05\n\n\nmax segments\nThe maximum number of segments allowed in trajectory fitting\n\n\n\nvertex count overshoot\nInitial regression-based detection of potential vertices can overshoot the number of vertices allowed by max segments by this value; angle-based culling is used to return to the desired number of vertices if overshoot occurs. This allows for a mix of criteria for vertex identification.\n3\n\n\nrecovery threshold\nDisallows candidate segment that have a recovery rate faster than 1/recovery threshold (in years).\n0.25\n\n\nprevent one year recovery\nDisallows all segments with recovery rates equal to one year, either true or false.\nFALSE\n\n\nbest model proportion\nAllows models with more vertices than allowed by max segments to be chosen if their p-value is no more than (2 – best model proportion) times the p-value of the best model.\n0.75\n\n\nminimum observations needed\nMinimum number of cloud free images needed to perform fitting.\n6\n\n\n\n\n\n\n\n\n\n\n\nThere is also a second set of parameters that are easily adjusted by the user that control which parts of the segmented time series are exported to a change map output. These parameters can greatly affect how your change map appears, and as such are important to understand. For more details on these parameters see the eMapR API documentation.\n\n\n\n\nLandTrendr Change Map Parameters\n\n\nParameter\nDescription\n\n\n\n\ndelta\nChange type to map – either loss (disturbance) or gain (recovery/growth).\n\n\nsort\nThe type of change to identify if there is more than one change event in the pixel of interest – one of greatest, least, newest, oldest, fastest, slowest.\n\n\nmagnitude\nEither a minimum or maximum value for filtering change events by change in spectral reflectance value; filtering is not required.\n\n\nduration\nEither a minimum or maximum value for filtering change events by length of event. Filtering by disturbance duration is not required.\n\n\npreval\nEither a minimum or maximum value for filtering change events by the pre-change value of the spectral index. Filtering by preval is not required.\n\n\nminimum mapping unit (mmu)\nIndicates the smallest allowable disturbance patch. Pixels disturbed in the same year and adjacent following an 8-neightbor rule are considered to be of the same patch. Filtering by mmu is not required.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "getting_started.html#selecting-imagery",
    "href": "getting_started.html#selecting-imagery",
    "title": "Getting Started",
    "section": "",
    "text": "One thing to be aware of when you are running LandTrendr is what imagery you are feeding into the algorithm. If you are using one of the eMapR scripts, the selection of imagery happens under the hood in a separate piece of code that is called to actually run the algorithm. As of April 2024, all the eMapR scripts default to Collection 2 Landsat imagery. For most users, who want to run Landtrendr on Landsat imagery, this is not a problem. However, if you want to use other timeseries data or if you are generating outputs over long periods of time, you may need to adjust this.\nAdditionally, if you are using the eMapR scripts, their API is designed to automatically be compatible with many common spectral indcies which makes it easy to switch between indices. In the CAFRI lab we almost always run LandTrendr on NBR, as we have found it to have the best performance.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "tuning.html",
    "href": "tuning.html",
    "title": "Tuning",
    "section": "",
    "text": "To improve the performance of the LandTrendr algorithm in the Northern Forest Region we leveraged a dataset of forest harvest records to create several parameter combinations, or tuning, that can be used specifically for the detection of harvest disturbances in the Northern Forest region. Tuning was conducted in two phases - initial parameter testing against single disturbance map outputs, and a second phase of tuning that tested performance against all detected disturbances. This is an overview of the methods used in this project (adapted from Desrochers 2024).\n\n\n\n\nThe initial tuning was intended to learn the effects of different parameters on algorithm outputs. Here we tested 28 different LandTrendr outputs and evaluated the performance of the tunings using four metrics: precision, recall, F1 score, and overall accuracy. The method of assessment was taken from Desrochers et al. (2022) where we looked for pixel-level match within a +/- 1 year period around the timing of harvest. Pixel states were assigned, tallied, and used to calculate performance metrics. A pixel where LandTrendr and the reference data agreed about a disturbance was a true positive (TP); a pixel where they both agreed about the lack of a disturbance was a true negative (TN). Disagreement was either a false negative (FN) in the case where the algorithm failed to detect a recorded disturbance, or a false positive (FP) in the case of disturbances detected where the reference data did not indicate that one had occurred.\nThe selected algorithm parameters for tuning included: max segments, vertex count overshoot, recovery threshold, p-value threshold, and best model proportion. These parameters were selected based on fitting suggestions in Kennedy et al. (2010). Other parameters were held constant at default values. We initially tested each parameter individually, only altering the value of one parameter at a time (Table 1). Once we had determined which parameter values were most effective at increasing algorithm accuracy on their own, we tested combinations of high performing parameters (Table 2).\n\n\n\n\nTable 1. Individual Parameter Values Tested\n\n\nparameter\nvalues tested\n\n\n\n\nmax segments\n8, 10, 12, 14, 16\n\n\nvertex count overshoot\n1, 3, 5\n\n\nrecovery threshold\n0.25, 0.5, 0.75, 0.8, 0.9, 1\n\n\np value threshold\n0.05, 0.1, 0.2\n\n\nbest model proportion\n0.5, 0.75, 1\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2. Tested Parameter Combinations\n\n\nparameter combination\nmax segments\nvertex count overshoot\nrecovery threshold\np value threshold\nbest model proportion\n\n\n\n\n1\n12\n1\n0.75\n0.10\n0.75\n\n\n2\n16\n1\n0.75\n0.10\n0.75\n\n\n3\n16\n5\n0.75\n0.10\n0.75\n\n\n4\n12\n5\n0.75\n0.10\n0.75\n\n\n5\n16\n5\n0.50\n0.10\n0.75\n\n\n6\n12\n5\n0.50\n0.10\n0.75\n\n\n7\n16\n5\n0.75\n0.20\n0.75\n\n\n8\n12\n5\n0.75\n0.20\n0.75\n\n\n9\n16\n5\n0.50\n0.05\n0.75\n\n\n10\n16\n1\n0.75\n0.05\n0.75\n\n\n11\n12\n1\n0.50\n0.10\n0.75\n\n\n12\n16\n1\n0.50\n0.10\n0.75\n\n\n\n\n\n\n\n\n\n\n\nThe combinations of algorithm parameters with the highest performance in this initial stage of tuning were carried over to the second stage of tuning and tested against all detected disturbances. A typical change map output produced by the Landtrendr algorihtm can identify at most one disturbance per pixel, however, it is possible, or even likely given a long enough time period, that a given pixel would have multiple disturbances over a given study period. To assess more complex disturbance outputs we utilized two additional LandTrendr outputs: 1) the vertices identified by the LT algorithm, and 2) the smoothed spectral trajectory fit to those vertices. Using these output layers, we extracted all pixels that had vertices associated with a decrease in spectral reflectance value. This approach was more inclusive of all possible disturbances, such that multiple disturbances could be identified at the pixel level. The output of this approach was a raster stack with one layer for each year in the study period, hereafter referred to as the ‘losses’ output.\nThis format, while useful, introduced some challenges for accuracy assessment. Assessing accuracy on a year-by-year basis was fairly straightforward as once we had created the losses output, it could be matched to the harvest records from that year in a manner similar to the method used in the initial tuning. However, those yearly accuracy values could not simply be summed to represent overall accuracy, as this would have overcounted the FN values within the harvested area, inflating that metric and overall negatively biasing the evaluation of algorithm performance. To illustrate this situation, consider a harvest that is 3×3 pixels (9 pixels total) and takes place over a single year in 2017. In our protocol, all harvests were given a +/- 1 year temporal buffer to account for detection lag and winter harvesting, which means that we considered disturbances in 2016, 2017 and 2018 for our sample harvest. If over those three years we observe in LandTrendr outputs a total of six TP pixels (e.g., one in 2016, two in 2017 and three in 2018) out of nine pixels, this may seem like good performance. However, this does not account for the FN pixels, which in this case would equal 21 (more than 3 times the TP count). A similar issue occurs outside the harvest polygons (but within the tract boundaries) with the TN class. Across all the harvests, this issue creates a bias that inflates the negative classes (FN and TN) and makes it difficult to reliably assess the accuracy of algorithm outputs.\nTo address this issue, we developed a new method that separately handles the area inside and outside of harvest polygons. The area within the harvest polygons was assessed first. To avoid overcounting FN, we assessed the two-pixel states relevant to harvest areas (TP and FN) on an individual harvest basis. For each harvest we extracted the layers corresponding to the years of harvest (plus the buffer years) from the losses output and counted the number of detected disturbances present within the boundary of the harvest; this number represents the number of TP for this harvest. The number of FN pixels was calculated by subtracting the number of TP from the total number of cells contained within the harvest boundaries. By this method we are taking an “overhead view” of the harvest and allowing all positively identified disturbance pixels to show through, which eliminates the overcounting of FN. Because the outcome of this method is a value that is equivalent to only the total number of pixels in one year within the harvest polygon, the values were multiplied by the number of years of harvest (plus buffer years) so that they would match more closely with outputs for the area outside of the polygons. The TP and FN counts from all harvest polygons were then summed to provide total values for the study period.\nThe area within the tract boundaries, but outside the harvest polygons was assessed on a yearly basis. Each year in the losses output was masked to remove the areas that were harvested that year. Because this remaining area by definition has no recorded harvests, FP counts can be determined by counting all disturbance pixels in each layer. After this step, TN counts were calculated by subtracting the number of FP from the total number of pixels in the layer, and then FP and TN counts can be summed across all years in the study period to give a total value. As before, we used precision, recall, accuracy and F1 score as performance metrics for comparing LandTrendr tuning outputs.\n\n\n\n\n\n\n\nTable 3. Accuracy metrics for all tested parameter combinations\n\n\nshort_name\nPrecision\nRecall\nF1\nAccuracy\n\n\n\n\nrecovery threshold 0.75\n0.6402340\n0.2963899\n0.4051976\n0.7795723\n\n\ndefaults\n0.6671112\n0.2214444\n0.3325127\n0.7738120\n\n\nmax segmetns 08\n0.6716189\n0.2192798\n0.3306155\n0.7739956\n\n\nmax segments 12\n0.6686132\n0.2242831\n0.3358926\n0.7743272\n\n\nvertex overshoot 1\n0.6724434\n0.2213153\n0.3330250\n0.7744001\n\n\nvertex overshoot 5\n0.6688639\n0.2228852\n0.3343538\n0.7741833\n\n\nrecovery threshold 0.5\n0.6818447\n0.2884402\n0.4053890\n0.7852595\n\n\npval 0.1\n0.6687672\n0.2236114\n0.3351581\n0.7743272\n\n\npval 0.2\n0.6719916\n0.2259091\n0.3381421\n0.7750529\n\n\nmodelproportion 1\n0.6691561\n0.2202954\n0.3314672\n0.7738704\n\n\nmodel proportion 0.5\n0.6691561\n0.2202954\n0.3314672\n0.7738704\n\n\nmax segments 16\n0.6686719\n0.2249838\n0.3366853\n0.7743876\n\n\nmax segments 14\n0.6671310\n0.2237440\n0.3351010\n0.7740706\n\n\nrecovery threshold 0.8\n0.6290196\n0.2976814\n0.4041162\n0.7777933\n\n\nrecovery threshold 0.9\n0.6134995\n0.2992147\n0.4022466\n0.7751551\n\n\nrecoverythreshold 1\n0.6026096\n0.3000611\n0.4006327\n0.7731905\n\n\ncombo 01\n0.6216088\n0.3016484\n0.4061864\n0.7771572\n\n\ncombo 02\n0.6077542\n0.3067283\n0.4076959\n0.7751593\n\n\ncombo 03\n0.6102023\n0.3073630\n0.4088071\n0.7756723\n\n\ncombo 04\n0.6266683\n0.3037326\n0.4091561\n0.7782667\n\n\ncombo 05\n0.6635782\n0.2978939\n0.4111942\n0.7839852\n\n\ncombo 06\n0.6750508\n0.2954267\n0.4109895\n0.7853387\n\n\ncombo 07\n0.6100671\n0.3087742\n0.4100229\n0.7757411\n\n\ncombo 08\n0.6259651\n0.3050847\n0.4102302\n0.7782959\n\n\ncombo 10\n0.6106240\n0.3066615\n0.4082804\n0.7756181\n\n\ncombo 09\n0.6679905\n0.2964055\n0.4106115\n0.7845004\n\n\ncombo 11\n0.6676184\n0.2945002\n0.4087100\n0.7841208\n\n\ncombo 12\n0.6602451\n0.2994984\n0.4120733\n0.7836015\n\n\n\n\n\n\n\n\nFigure 1. Comparison of accuracy assessment metrics for algorithm tunings.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThrough this tuning process we found the recovery threshold, max segments, the p-value threshold parameters to be the most important for improving the detection of harvest disturbances in the Northern Forest Region. In selecting the ‘best’ tuning, we looked for parameter combinations that maximized F1 and recall, while minimizing the decrease in precision (Figure 1) . The parameter set that yielded best overall improvement of performance metrics was ‘combination 6’. This group of parameters yielded outputs with the highest overall accuracy and third-highest F1 score, while maintaining relatively high precision (Table 3). Other parameter combinations could be selected to maximize specific metrics, as follows: combination 12 for F1, recovery threshold 0.5 for precision, combination 7 for recall.",
    "crumbs": [
      "Tuning"
    ]
  },
  {
    "objectID": "tuning.html#methods",
    "href": "tuning.html#methods",
    "title": "Tuning",
    "section": "",
    "text": "The initial tuning was intended to learn the effects of different parameters on algorithm outputs. Here we tested 28 different LandTrendr outputs and evaluated the performance of the tunings using four metrics: precision, recall, F1 score, and overall accuracy. The method of assessment was taken from Desrochers et al. (2022) where we looked for pixel-level match within a +/- 1 year period around the timing of harvest. Pixel states were assigned, tallied, and used to calculate performance metrics. A pixel where LandTrendr and the reference data agreed about a disturbance was a true positive (TP); a pixel where they both agreed about the lack of a disturbance was a true negative (TN). Disagreement was either a false negative (FN) in the case where the algorithm failed to detect a recorded disturbance, or a false positive (FP) in the case of disturbances detected where the reference data did not indicate that one had occurred.\nThe selected algorithm parameters for tuning included: max segments, vertex count overshoot, recovery threshold, p-value threshold, and best model proportion. These parameters were selected based on fitting suggestions in Kennedy et al. (2010). Other parameters were held constant at default values. We initially tested each parameter individually, only altering the value of one parameter at a time (Table 1). Once we had determined which parameter values were most effective at increasing algorithm accuracy on their own, we tested combinations of high performing parameters (Table 2).\n\n\n\n\nTable 1. Individual Parameter Values Tested\n\n\nparameter\nvalues tested\n\n\n\n\nmax segments\n8, 10, 12, 14, 16\n\n\nvertex count overshoot\n1, 3, 5\n\n\nrecovery threshold\n0.25, 0.5, 0.75, 0.8, 0.9, 1\n\n\np value threshold\n0.05, 0.1, 0.2\n\n\nbest model proportion\n0.5, 0.75, 1\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2. Tested Parameter Combinations\n\n\nparameter combination\nmax segments\nvertex count overshoot\nrecovery threshold\np value threshold\nbest model proportion\n\n\n\n\n1\n12\n1\n0.75\n0.10\n0.75\n\n\n2\n16\n1\n0.75\n0.10\n0.75\n\n\n3\n16\n5\n0.75\n0.10\n0.75\n\n\n4\n12\n5\n0.75\n0.10\n0.75\n\n\n5\n16\n5\n0.50\n0.10\n0.75\n\n\n6\n12\n5\n0.50\n0.10\n0.75\n\n\n7\n16\n5\n0.75\n0.20\n0.75\n\n\n8\n12\n5\n0.75\n0.20\n0.75\n\n\n9\n16\n5\n0.50\n0.05\n0.75\n\n\n10\n16\n1\n0.75\n0.05\n0.75\n\n\n11\n12\n1\n0.50\n0.10\n0.75\n\n\n12\n16\n1\n0.50\n0.10\n0.75\n\n\n\n\n\n\n\n\n\n\n\nThe combinations of algorithm parameters with the highest performance in this initial stage of tuning were carried over to the second stage of tuning and tested against all detected disturbances. A typical change map output produced by the Landtrendr algorihtm can identify at most one disturbance per pixel, however, it is possible, or even likely given a long enough time period, that a given pixel would have multiple disturbances over a given study period. To assess more complex disturbance outputs we utilized two additional LandTrendr outputs: 1) the vertices identified by the LT algorithm, and 2) the smoothed spectral trajectory fit to those vertices. Using these output layers, we extracted all pixels that had vertices associated with a decrease in spectral reflectance value. This approach was more inclusive of all possible disturbances, such that multiple disturbances could be identified at the pixel level. The output of this approach was a raster stack with one layer for each year in the study period, hereafter referred to as the ‘losses’ output.\nThis format, while useful, introduced some challenges for accuracy assessment. Assessing accuracy on a year-by-year basis was fairly straightforward as once we had created the losses output, it could be matched to the harvest records from that year in a manner similar to the method used in the initial tuning. However, those yearly accuracy values could not simply be summed to represent overall accuracy, as this would have overcounted the FN values within the harvested area, inflating that metric and overall negatively biasing the evaluation of algorithm performance. To illustrate this situation, consider a harvest that is 3×3 pixels (9 pixels total) and takes place over a single year in 2017. In our protocol, all harvests were given a +/- 1 year temporal buffer to account for detection lag and winter harvesting, which means that we considered disturbances in 2016, 2017 and 2018 for our sample harvest. If over those three years we observe in LandTrendr outputs a total of six TP pixels (e.g., one in 2016, two in 2017 and three in 2018) out of nine pixels, this may seem like good performance. However, this does not account for the FN pixels, which in this case would equal 21 (more than 3 times the TP count). A similar issue occurs outside the harvest polygons (but within the tract boundaries) with the TN class. Across all the harvests, this issue creates a bias that inflates the negative classes (FN and TN) and makes it difficult to reliably assess the accuracy of algorithm outputs.\nTo address this issue, we developed a new method that separately handles the area inside and outside of harvest polygons. The area within the harvest polygons was assessed first. To avoid overcounting FN, we assessed the two-pixel states relevant to harvest areas (TP and FN) on an individual harvest basis. For each harvest we extracted the layers corresponding to the years of harvest (plus the buffer years) from the losses output and counted the number of detected disturbances present within the boundary of the harvest; this number represents the number of TP for this harvest. The number of FN pixels was calculated by subtracting the number of TP from the total number of cells contained within the harvest boundaries. By this method we are taking an “overhead view” of the harvest and allowing all positively identified disturbance pixels to show through, which eliminates the overcounting of FN. Because the outcome of this method is a value that is equivalent to only the total number of pixels in one year within the harvest polygon, the values were multiplied by the number of years of harvest (plus buffer years) so that they would match more closely with outputs for the area outside of the polygons. The TP and FN counts from all harvest polygons were then summed to provide total values for the study period.\nThe area within the tract boundaries, but outside the harvest polygons was assessed on a yearly basis. Each year in the losses output was masked to remove the areas that were harvested that year. Because this remaining area by definition has no recorded harvests, FP counts can be determined by counting all disturbance pixels in each layer. After this step, TN counts were calculated by subtracting the number of FP from the total number of pixels in the layer, and then FP and TN counts can be summed across all years in the study period to give a total value. As before, we used precision, recall, accuracy and F1 score as performance metrics for comparing LandTrendr tuning outputs.\n\n\n\n\n\n\n\nTable 3. Accuracy metrics for all tested parameter combinations\n\n\nshort_name\nPrecision\nRecall\nF1\nAccuracy\n\n\n\n\nrecovery threshold 0.75\n0.6402340\n0.2963899\n0.4051976\n0.7795723\n\n\ndefaults\n0.6671112\n0.2214444\n0.3325127\n0.7738120\n\n\nmax segmetns 08\n0.6716189\n0.2192798\n0.3306155\n0.7739956\n\n\nmax segments 12\n0.6686132\n0.2242831\n0.3358926\n0.7743272\n\n\nvertex overshoot 1\n0.6724434\n0.2213153\n0.3330250\n0.7744001\n\n\nvertex overshoot 5\n0.6688639\n0.2228852\n0.3343538\n0.7741833\n\n\nrecovery threshold 0.5\n0.6818447\n0.2884402\n0.4053890\n0.7852595\n\n\npval 0.1\n0.6687672\n0.2236114\n0.3351581\n0.7743272\n\n\npval 0.2\n0.6719916\n0.2259091\n0.3381421\n0.7750529\n\n\nmodelproportion 1\n0.6691561\n0.2202954\n0.3314672\n0.7738704\n\n\nmodel proportion 0.5\n0.6691561\n0.2202954\n0.3314672\n0.7738704\n\n\nmax segments 16\n0.6686719\n0.2249838\n0.3366853\n0.7743876\n\n\nmax segments 14\n0.6671310\n0.2237440\n0.3351010\n0.7740706\n\n\nrecovery threshold 0.8\n0.6290196\n0.2976814\n0.4041162\n0.7777933\n\n\nrecovery threshold 0.9\n0.6134995\n0.2992147\n0.4022466\n0.7751551\n\n\nrecoverythreshold 1\n0.6026096\n0.3000611\n0.4006327\n0.7731905\n\n\ncombo 01\n0.6216088\n0.3016484\n0.4061864\n0.7771572\n\n\ncombo 02\n0.6077542\n0.3067283\n0.4076959\n0.7751593\n\n\ncombo 03\n0.6102023\n0.3073630\n0.4088071\n0.7756723\n\n\ncombo 04\n0.6266683\n0.3037326\n0.4091561\n0.7782667\n\n\ncombo 05\n0.6635782\n0.2978939\n0.4111942\n0.7839852\n\n\ncombo 06\n0.6750508\n0.2954267\n0.4109895\n0.7853387\n\n\ncombo 07\n0.6100671\n0.3087742\n0.4100229\n0.7757411\n\n\ncombo 08\n0.6259651\n0.3050847\n0.4102302\n0.7782959\n\n\ncombo 10\n0.6106240\n0.3066615\n0.4082804\n0.7756181\n\n\ncombo 09\n0.6679905\n0.2964055\n0.4106115\n0.7845004\n\n\ncombo 11\n0.6676184\n0.2945002\n0.4087100\n0.7841208\n\n\ncombo 12\n0.6602451\n0.2994984\n0.4120733\n0.7836015\n\n\n\n\n\n\n\n\nFigure 1. Comparison of accuracy assessment metrics for algorithm tunings.",
    "crumbs": [
      "Tuning"
    ]
  },
  {
    "objectID": "tuning.html#recomended-tunings",
    "href": "tuning.html#recomended-tunings",
    "title": "Tuning",
    "section": "",
    "text": "Through this tuning process we found the recovery threshold, max segments, the p-value threshold parameters to be the most important for improving the detection of harvest disturbances in the Northern Forest Region. In selecting the ‘best’ tuning, we looked for parameter combinations that maximized F1 and recall, while minimizing the decrease in precision (Figure 1) . The parameter set that yielded best overall improvement of performance metrics was ‘combination 6’. This group of parameters yielded outputs with the highest overall accuracy and third-highest F1 score, while maintaining relatively high precision (Table 3). Other parameter combinations could be selected to maximize specific metrics, as follows: combination 12 for F1, recovery threshold 0.5 for precision, combination 7 for recall.",
    "crumbs": [
      "Tuning"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nThis document is a collection of resources for using the LandTrendr change detection algorithm. While it is by no means a comprehensive overveiw of all of the information available on this popular algorithm, it can hopefully serve as a jumping off point for expoloration of the many applications of this algorithm. Additionally, it serves as a discussion of the tuning work undertaken by the Climate and Applied Forest Research Institue at SUNY ESF made possible through funding from the Forest Ecosystem Monitoring Collective (FEMC).",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "code.html",
    "href": "code.html",
    "title": "Code",
    "section": "",
    "text": "The scripts that we used to generate LandTrendr outputs are heavily based on the eMapR API. They are also available in the git hub repo for this document.\n\nchange map outpus\nftv and vertex outputs\n\n\n\n\nWe have also included in that git hub repo several scripts that we used to process and assess the LandTrendr outputs for our tuning work.\n\nInitial tuning\nSecondary tuning - this script requires outputs from both this (processes the ftv outputs) and this (rasterizes reference data)",
    "crumbs": [
      "Code"
    ]
  },
  {
    "objectID": "code.html#google-earth-engine",
    "href": "code.html#google-earth-engine",
    "title": "Code",
    "section": "",
    "text": "The scripts that we used to generate LandTrendr outputs are heavily based on the eMapR API. They are also available in the git hub repo for this document.\n\nchange map outpus\nftv and vertex outputs",
    "crumbs": [
      "Code"
    ]
  },
  {
    "objectID": "code.html#accuracy-assesment",
    "href": "code.html#accuracy-assesment",
    "title": "Code",
    "section": "",
    "text": "We have also included in that git hub repo several scripts that we used to process and assess the LandTrendr outputs for our tuning work.\n\nInitial tuning\nSecondary tuning - this script requires outputs from both this (processes the ftv outputs) and this (rasterizes reference data)",
    "crumbs": [
      "Code"
    ]
  },
  {
    "objectID": "timesync.html",
    "href": "timesync.html",
    "title": "Time Sync",
    "section": "",
    "text": "Checking the accuracy of your LandTrendr outputs requires a separate set of forest disturbance data. LandTrendr was developed with a companion program for developing reference data called TimeSync (Cohen et al. 2010). For CAFRI members in the past, gaining access to this program has been a challenging, requiring lots of chasing the system administrators around by email. Using the program can also be some frustrating, as it doesn’t allow for any customization to the labeling process, however we have not found or created a better alternative yet (maybe you will!). The eMapR group does have a desktop version for windows computers available, but this has not yet been tested by any CAFRI personnel.\n\n\nOnce you are able to set up a timesync project, using the software is fairly straight forward. The user manual is the best available instructional document that we have found. The mentioned online tutorial with examples no longer appears to exist. Some disturbance types are difficult to distinguish based on spectral trajectory alone, and we found it to be helpful to have auxillery imagery to compare with the data presented in timesync. For this imagery we used the historical imagery in the Desktop version of Google Earth. This interface was easy to use, allows the user to import shapefiles (ie. your timesync plot locaions), and requires no local hosting of imagery. However, the temporal frequency of this imagery is not ideal, with gaps of 10+ years in some locations.\nOne final note, any comments made within the TimeSync platform do not export with your final data, so keep that in mind if you are leaving commments that are important for the interpretation of your final labeled trajectories.\n\n\n\nThe only alternative to TimeSync that we have come across is CollectEarth software. In our exploration this software was much more customizable, but lacked the tailored timeseries comparisons that makes TimeSync work so well with LandTrendr. CollectEarth does have many more imagery options, so could be used as a source of auxillery refernce imagery.",
    "crumbs": [
      "Time Sync"
    ]
  },
  {
    "objectID": "timesync.html#using-timesync",
    "href": "timesync.html#using-timesync",
    "title": "Time Sync",
    "section": "",
    "text": "Once you are able to set up a timesync project, using the software is fairly straight forward. The user manual is the best available instructional document that we have found. The mentioned online tutorial with examples no longer appears to exist. Some disturbance types are difficult to distinguish based on spectral trajectory alone, and we found it to be helpful to have auxillery imagery to compare with the data presented in timesync. For this imagery we used the historical imagery in the Desktop version of Google Earth. This interface was easy to use, allows the user to import shapefiles (ie. your timesync plot locaions), and requires no local hosting of imagery. However, the temporal frequency of this imagery is not ideal, with gaps of 10+ years in some locations.\nOne final note, any comments made within the TimeSync platform do not export with your final data, so keep that in mind if you are leaving commments that are important for the interpretation of your final labeled trajectories.",
    "crumbs": [
      "Time Sync"
    ]
  },
  {
    "objectID": "timesync.html#alternatives-to-timesync",
    "href": "timesync.html#alternatives-to-timesync",
    "title": "Time Sync",
    "section": "",
    "text": "The only alternative to TimeSync that we have come across is CollectEarth software. In our exploration this software was much more customizable, but lacked the tailored timeseries comparisons that makes TimeSync work so well with LandTrendr. CollectEarth does have many more imagery options, so could be used as a source of auxillery refernce imagery.",
    "crumbs": [
      "Time Sync"
    ]
  }
]